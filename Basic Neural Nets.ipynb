{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a neural network that is trained on dummy data. Here's is our data below. Each training example's features are either 1 or 0, and you'll notice that the label is always exactly equal to the first feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 1, 0, 1], [0, 1, 0, 1], [0, 1, 0, 1], [1, 0, 1, 0],\n",
    "              [0, 1, 1, 0], [1, 0, 1, 1], [0, 0, 0, 0], [1, 1, 1, 0],\n",
    "              [0, 0, 1, 1], [1, 1, 0, 1], [0, 0, 1, 0], [1, 0, 0, 0],\n",
    "              [1, 1, 1, 1], [0, 1, 1, 1], [1, 0, 0, 1], [1, 0, 0, 1]])\n",
    "y = np.array([[0], [0], [0], [1], [1], [1], [0], [1], [1], [0], [1], [0], [1], [1], [0], [0]])\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split this up into the train, validation, and test datasets. Make it a 50:25:25 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = None\n",
    "X_val = None\n",
    "X_test = None\n",
    "\n",
    "y_train = None\n",
    "y_val = None\n",
    "y_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a simple 1-layer fully-connected neural network, as you practiced in the previous notebook. A fully connected notebook consists of a multiplicative weight matrix and an additive bias vector. Intialize the weight to random values and bias zeros. Make sure that the shapes of the two are correct to transform each training example from having 4 elements (for each feature) to 2 (one for each label i.e. what the network predicts the answer to be)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = None\n",
    "b = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a function that acts as the linear layer for the network. The equations for the layer is:\n",
    "\n",
    "$$o = Mx + b$$\n",
    "\n",
    "Don't worry about the 'linear_cache' variable for now, we'll discuss that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_cache = {}\n",
    "def linear(input):\n",
    "    output = None\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear layer will output a vector with 2 elements. The 0th element will contain a 1 if the network thinks the correct label is 0, and the 1st element will contain a 1 if the network thinks the correct label is 1 (this is called one-hot encoding).\n",
    "\n",
    "This is great, but the output will be arbitrarily scaled. We want to turn the output into a probability distribution. So, let's implement a softmax cross-entropy layer. The equation for softmax is:\n",
    "\n",
    "$$p_j = \\frac{e^{f_j}}{\\sum_{i=1}^ne^{f_i}}$$\n",
    "\n",
    "And the equation for cross-entropy is:\n",
    "$$l = -\\log(p_c)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cache = {}\n",
    "def softmax_cross_entropy(input, y):\n",
    "    output = None\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed during the lecture, implement the backward pass for the softmax and linear layers. Modify your above code to include variables in the caches as necessary. Note: the equation for the SCE gradient is:\n",
    "\n",
    "$$(\\nabla p)_j = \\frac{e^{f_j}}{\\sum_{i=1}^ne^{f_i}} 1\\{j == c\\} = p_j - 1\\{j == c\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_backward():\n",
    "    dloss = None\n",
    "    return dloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dout):\n",
    "    dW, db = None, None\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our network! We provided below a useful function to test the accuracy of your network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(output, target):\n",
    "    pred = np.argmax(output, axis=1)\n",
    "    target = np.reshape(target, (target.shape[0]))\n",
    "    correct = np.sum(pred == target)\n",
    "    accuracy = correct / pred.shape[0] * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create your training regime which 1) samples a batch of training data 2) passes the batch forward through the network and computes loss 3) backpropigates into the weight and bias 4) updates the weights and 5) periodically outputs the accuracy on the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4000):\n",
    "    # Get a batch of data\n",
    "\n",
    "    # Forward Pass\n",
    "\n",
    "    # Backward Pass\n",
    "\n",
    "    # Weight updates\n",
    "\n",
    "    # Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, check your accuracy on the test set to see how you did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = eval_accuracy(linear(X_test), y_test)\n",
    "print(\"Test Accuracy: %f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
